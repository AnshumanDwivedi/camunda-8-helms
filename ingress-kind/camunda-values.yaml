# Chart values for the Camunda Platform 8 Helm chart.
# This file deliberately contains only the values that differ from the defaults.
# For changes and documentation, use your favorite diff tool to compare it with:
# https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform/values.yaml

# This is a very small cluster useful for running locally and for development

global:
  identity:
    auth:
      publicIssuerUrl: "http://keycloak.127.0.0.1.nip.io/auth/realms/camunda-platform"
      operate:
        redirectUrl: "http://operate.127.0.0.1.nip.io"
      tasklist:
        redirectUrl: "http://tasklist.127.0.0.1.nip.io"
      optimize:
        redirectUrl: "http://optimize.127.0.0.1.nip.io"
operate:
  enabled: true
  ingress:
    enabled: true
    host: "operate.127.0.0.1.nip.io"
tasklist:
  enabled: true
  ingress:
    enabled: true
    host: "tasklist.127.0.0.1.nip.io"
optimize:
  enabled: true
  env:
   - name: CAMUNDA_OPTIMIZE_EMAIL_ACCESS_URL
     value: "http://optimize.127.0.0.1.nip.io"
  ingress:
    enabled: true
    host: "optimize.127.0.0.1.nip.io"
identity:
  enabled: true
  env:
   - name: IDENTITY_URL
     value: "http://identity.127.0.0.1.nip.io"
  keycloak:
    ingress:
      enabled: true
      ingressClassName: nginx
      hostname: "keycloak.127.0.0.1.nip.io"
    extraEnvVars:
      - name: KEYCLOAK_PROXY_ADDRESS_FORWARDING
        value: "true"
      - name: KEYCLOAK_FRONTEND_URL
        value: "http://keycloak.127.0.0.1.nip.io"
  ingress:
    enabled: true
    host: "identity.127.0.0.1.nip.io"

zeebe:
  clusterSize: 1
  partitionCount: 1
  replicationFactor: 1
  pvcSize: 1Gi

  resources:
    requests:
      cpu: "100m"
      memory: "512M"
    limits:
      cpu: "512m"
      memory: "2Gi"

zeebe-gateway:
  replicas: 1

  resources:
    requests:
      cpu: "100m"
      memory: "512M"
    limits:
      cpu: "1000m"
      memory: "1Gi"

  logLevel: ERROR

elasticsearch:
  enabled: true
  imageTag: 7.17.3
  replicas: 1
  minimumMasterNodes: 1
  # Allow no backup for single node setups
  clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

  resources:
    requests:
      cpu: "100m"
      memory: "512M"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  # Request smaller persistent volumes.
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: "standard"
    resources:
      requests:
        storage: 15Gi
