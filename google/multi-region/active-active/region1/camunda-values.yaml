# Chart values for the Camunda Platform 8 Helm chart.
# This file deliberately contains only the values that differ from the defaults.
# For changes and documentation, use your favorite diff tool to compare it with:
# https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform/values.yaml

global:
  # Multiregion options for Zeebe
  #
  ## WARNING: In order to get your multi-region setup covered by Camunda enterprise support you MUST get your configuration and run books reviewed by Camunda before going to production.
  # This is necessary for us to be able to help you in case of outages, due to the complexity of operating multi-region setups and the dependencies to the underlying Kubernetes prerequisites.
  # If you operate this in the wrong way you risk corruption and complete loss of all data especially in the dual-region case.
  # If you can, consider three regions. Please, contact your customer success manager as soon as you start planning a multi-region setup.
  # Camunda reserves the right to limit support if no review was done prior to launch or the review showed significant risks.
  multiregion:
    # number of regions that this Camunda Platform instance is stretched across
    regions: 2
    # unique id of the region. Should start at 0 for easy computation. With 2 regions, you would have region 0 and 1.
    regionId: 1
  identity:
    auth:
      # Disable the Identity authentication
      # it will fall back to basic-auth: demo/demo as default user
      enabled: false

operate:
  enabled: false

tasklist:
  enabled: false
      
identity:
  enabled: false

optimize:
  enabled: false

connectors:
  enabled: false
  inbound:
    mode: credentials
  resources:
    requests:
      cpu: "100m"
      memory: "512M"
    limits:
      cpu: "1000m"
      memory: "2Gi"
  env:
    - name: CAMUNDA_OPERATE_CLIENT_USERNAME
      value: demo
    - name: CAMUNDA_OPERATE_CLIENT_PASSWORD
      value: demo

zeebe:
  clusterSize: 8
  partitionCount: 8
  replicationFactor: 4
  env:
    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD
      value: "5m"
    - name: ZEEBE_BROKER_DATA_DISKUSAGECOMMANDWATERMARK
      value: "0.85"
    - name: ZEEBE_BROKER_DATA_DISKUSAGEREPLICATIONWATERMARK
      value: "0.87"
    - name: ZEEBE_BROKER_CLUSTER_INITIALCONTACTPOINTS
      value: "camunda-zeebe-0.camunda-zeebe.us-east1.svc.cluster.local:26502,camunda-zeebe-1.camunda-zeebe.us-east1.svc.cluster.local:26502,camunda-zeebe-2.camunda-zeebe.us-east1.svc.cluster.local:26502,camunda-zeebe-3.camunda-zeebe.us-east1.svc.cluster.local:26502,camunda-zeebe-0.camunda-zeebe.europe-west1.svc.cluster.local:26502,camunda-zeebe-1.camunda-zeebe.europe-west1.svc.cluster.local:26502,camunda-zeebe-2.camunda-zeebe.europe-west1.svc.cluster.local:26502,camunda-zeebe-3.camunda-zeebe.europe-west1.svc.cluster.local:26502"
    - name: ZEEBE_BROKER_THREADS_IOTHREADCOUNT
      value: "4"
    - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH2_CLASSNAME
      value: "io.camunda.zeebe.exporter.ElasticsearchExporter"
    - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH2_ARGS_URL
      value: "http://camunda-elasticsearch.us-east1.svc.cluster.local:9200"
    - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH2_ARGS_INDEX_PREFIX
      value: "zeebe-record"
    - name: ZEEBE_BROKER_CLUSTER_MESSAGECOMPRESSION
      value: "GZIP"
    - name: ZEEBE_BROKER_CLUSTER_MEMBERSHIP_PROBETIMEOUT
      value: "1s"
    - name: ZEEBE_BROKER_BACKPRESSURE_AIMD_REQUESTTIMEOUT
      value: "1s"
  pvcSize: 64Gi

  resources:
    requests:
      cpu: "1350m"
      memory: "4Gi"
    limits:
      cpu: "1350m"
      memory: "4Gi"

zeebe-gateway:
  replicas: 1
  env: 
    - name: ZEEBE_GATEWAY_CLUSTER_MESSAGECOMPRESSION
      value: "GZIP"
    - name: ZEEBE_GATEWAY_CLUSTER_MEMBERSHIP_PROBEINTERVAL
      value: "1s"

  resources:
    requests:
      cpu: "100m"
      memory: "512M"
    limits:
      cpu: "1000m"
      memory: "1Gi"

  logLevel: ERROR

elasticsearch:
  enabled: true
  master:
    resources:
      request:
        cpu: 6
        memory: "6Gi"
      limit: 
        cpu: 6
        memory: "6Gi"
#  imageTag: 7.17.3
  replicas: 1
 
  # Allow no backup for single node setups
  clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

  resources:
    requests:
      cpu: 6
      memory: "6Gi"
    limits:
      cpu: 12
      memory: "12Gi"

  # Request smaller persistent volumes.
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: "standard"
    resources:
      requests:
        storage: 15Gi
prometheusServiceMonitor:
  enabled: true
  labels:
    release: monitoring
  scrapeInterval: 30s

prometheus-elasticsearch-exporter:
  es:
    ## Address (host and port) of the Elasticsearch node we should connect to.
    ## This could be a local node (localhost:9200, for instance), or the address
    ## of a remote Elasticsearch server. When basic auth is needed,
    ## specify as: <proto>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200.
    ##
    uri: "http://{{ .Release.Name }}-elasticsearch:9200"
  serviceMonitor:
    ## If true, a ServiceMonitor CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: true
    # Labels used by the service monitor, specific to our prometheus installation
    labels:
      release: monitoring